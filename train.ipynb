{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1RGLVsaX51TitroCKTu2KMkivTmFiCtS6",
      "authorship_tag": "ABX9TyPBcifDagQVBr1LyNfj54ZW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isabella-001/photofolio-app/blob/main/train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHu_NTJMF0YX",
        "outputId": "66278642-7422-49f6-9b97-91a8b65ab229"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (0.23.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (6.0.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb) (8.3.1)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (3.1.45)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb) (4.5.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.12.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.47.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.12/dist-packages (from wandb) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets wandb pyyaml torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Log in to Weights & Biases (Optional but recommended for tracking)\n",
        "import wandb\n",
        "wandb.login()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIlMT0cKGRX3",
        "outputId": "905edae3-699a-402e-cfb7-fc0f578df1a6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify TPU/GPU availability\n",
        "import torch\n",
        "print(f\"Device: {torch.device('cuda' if torch.cuda.is_available() else 'cpu')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6eB6HBbGflw",
        "outputId": "615c6557-2242-407b-c8b8-93e0bfb34acd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers \"datasets<3.0.0\" wandb pyyaml torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khb9CpBGGk8w",
        "outputId": "661ffe5f-11f9-4436-a2c2-7be9d4142676"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
            "Collecting datasets<3.0.0\n",
            "  Downloading datasets-2.21.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (0.23.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (6.0.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets<3.0.0) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets<3.0.0) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets<3.0.0) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets<3.0.0) (3.6.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from datasets<3.0.0) (0.70.16)\n",
            "Collecting fsspec<=2024.6.1,>=2023.1.0 (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets<3.0.0)\n",
            "  Downloading fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from datasets<3.0.0) (3.13.2)\n",
            "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb) (8.3.1)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (3.1.45)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb) (4.5.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.12.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.47.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.12/dist-packages (from wandb) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets<3.0.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets<3.0.0) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets<3.0.0) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets<3.0.0) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets<3.0.0) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets<3.0.0) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets<3.0.0) (1.22.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets<3.0.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets<3.0.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets<3.0.0) (2025.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets<3.0.0) (1.17.0)\n",
            "Downloading datasets-2.21.0-py3-none-any.whl (527 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m527.3/527.3 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fsspec, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.0\n",
            "    Uninstalling fsspec-2025.3.0:\n",
            "      Successfully uninstalled fsspec-2025.3.0\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 4.0.0\n",
            "    Uninstalling datasets-4.0.0:\n",
            "      Successfully uninstalled datasets-4.0.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.6.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.21.0 fsspec-2024.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Log in to Weights & Biases (Optional but recommended for tracking)\n",
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qXfvw0lHnE0",
        "outputId": "c4376e6c-05b9-45ea-8cdf-e95ce718f445"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify TPU/GPU availability\n",
        "import torch\n",
        "print(f\"Device: {torch.device('cuda' if torch.cuda.is_available() else 'cpu')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ruv4mNDHpe-",
        "outputId": "cd040d3f-ace8-4cce-9555-0eb7efad40bc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the WikiSQL Dataset\n",
        "# This script handles downloading through the `datasets` library.\n",
        "!python download_data.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1AJ1S9VHvSm",
        "outputId": "1780888c-8b06-455b-cfc2-780d7cd56c9a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking dependencies...\n",
            "Downloading WikiSQL dataset from Hugging Face...\n",
            "Downloading builder script: 100% 6.57k/6.57k [00:00<00:00, 26.6kB/s]\n",
            "Downloading readme: 100% 7.80k/7.80k [00:00<00:00, 23.6kB/s]\n",
            "Downloading data: 100% 26.2M/26.2M [00:00<00:00, 99.7MB/s]\n",
            "Generating test split: 100% 15878/15878 [00:03<00:00, 4725.68 examples/s]\n",
            "Generating validation split: 100% 8421/8421 [00:01<00:00, 5035.34 examples/s]\n",
            "Generating train split: 100% 56355/56355 [00:13<00:00, 4067.55 examples/s]\n",
            "Processing train -> MyTrials/train.json...\n",
            "100% 56355/56355 [00:24<00:00, 2307.07it/s]\n",
            "Processing validation -> MyTrials/validation.json...\n",
            "100% 8421/8421 [00:03<00:00, 2517.75it/s]\n",
            "Processing test -> MyTrials/test.json...\n",
            "100% 15878/15878 [00:07<00:00, 2157.85it/s]\n",
            "Done! Data saved to MyTrials\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start Training\n",
        "# This runs the training loop defined in train.py\n",
        "!python train.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLyGYNSII2qn",
        "outputId": "1793de3d-6b46-497c-fa38-58d75134b710"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train data path: /content/MyTrials/train.json\n",
            "tokenizer_config.json: 100% 2.32k/2.32k [00:00<00:00, 14.9MB/s]\n",
            "spiece.model: 100% 792k/792k [00:00<00:00, 4.77MB/s]\n",
            "tokenizer.json: 100% 1.39M/1.39M [00:00<00:00, 5.66MB/s]\n",
            "Creating new model...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtahasanhabib83\u001b[0m (\u001b[33mtahasanhabib83-rajshahi-university-of-engineering-technology\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m Waiting for wandb.init()...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m Waiting for wandb.init()...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£½\u001b[0m setting up run 8v0l0188 (0.2s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¾\u001b[0m setting up run 8v0l0188 (0.2s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.23.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20251214_135359-8v0l0188\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mgood-meadow-1\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/tahasanhabib83-rajshahi-university-of-engineering-technology/transformer-sql-fixed\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/tahasanhabib83-rajshahi-university-of-engineering-technology/transformer-sql-fixed/runs/8v0l0188\u001b[0m\n",
            "\n",
            "the device is:  cuda\n",
            "Total trainable parameters: 27,475,200\n",
            "Training for 15 epochs\n",
            "\n",
            "Epoch 1/15 | Batch 100/1762 | Loss: 6.5907 | Avg: 8.7033 | LR: 0.000050 | GradNorm: 2.3666\n",
            "Epoch 1/15 | Batch 200/1762 | Loss: 4.8043 | Avg: 7.1950 | LR: 0.000100 | GradNorm: 0.9145\n",
            "Epoch 1/15 | Batch 300/1762 | Loss: 4.3480 | Avg: 6.3196 | LR: 0.000150 | GradNorm: 1.1149\n",
            "Epoch 1/15 | Batch 400/1762 | Loss: 4.2772 | Avg: 5.8101 | LR: 0.000200 | GradNorm: 1.1664\n",
            "Epoch 1/15 | Batch 500/1762 | Loss: 3.9874 | Avg: 5.4719 | LR: 0.000250 | GradNorm: 1.3405\n",
            "Epoch 1/15 | Batch 600/1762 | Loss: 3.5860 | Avg: 5.2219 | LR: 0.000300 | GradNorm: 1.0393\n",
            "Epoch 1/15 | Batch 700/1762 | Loss: 3.9869 | Avg: 5.0317 | LR: 0.000350 | GradNorm: 1.2602\n",
            "Epoch 1/15 | Batch 800/1762 | Loss: 4.0682 | Avg: 4.8741 | LR: 0.000400 | GradNorm: 1.1927\n",
            "Epoch 1/15 | Batch 900/1762 | Loss: 3.6874 | Avg: 4.7429 | LR: 0.000450 | GradNorm: 1.2285\n",
            "Epoch 1/15 | Batch 1000/1762 | Loss: 3.4975 | Avg: 4.6290 | LR: 0.000500 | GradNorm: 1.3579\n",
            "Epoch 1/15 | Batch 1100/1762 | Loss: 3.5544 | Avg: 4.5329 | LR: 0.000500 | GradNorm: 1.5033\n",
            "Epoch 1/15 | Batch 1200/1762 | Loss: 3.4271 | Avg: 4.4458 | LR: 0.000500 | GradNorm: 1.6848\n",
            "Epoch 1/15 | Batch 1300/1762 | Loss: 3.1474 | Avg: 4.3620 | LR: 0.000500 | GradNorm: 1.2067\n",
            "Epoch 1/15 | Batch 1400/1762 | Loss: 3.3994 | Avg: 4.2901 | LR: 0.000500 | GradNorm: 1.1179\n",
            "Epoch 1/15 | Batch 1500/1762 | Loss: 3.3561 | Avg: 4.2247 | LR: 0.000500 | GradNorm: 1.3771\n",
            "Epoch 1/15 | Batch 1600/1762 | Loss: 3.0975 | Avg: 4.1645 | LR: 0.000500 | GradNorm: 1.2849\n",
            "Epoch 1/15 | Batch 1700/1762 | Loss: 3.3664 | Avg: 4.1090 | LR: 0.000500 | GradNorm: 1.1357\n",
            "\n",
            "============================================================\n",
            "Epoch 1/15 | Average Loss: 4.0783\n",
            "best loss found so far.\n",
            "============================================================\n",
            "\n",
            "Checkpoint saved to /content/transformer_model.pt\n",
            "\n",
            "Epoch 2/15 | Batch 100/1762 | Loss: 3.0824 | Avg: 3.1031 | LR: 0.000500 | GradNorm: 1.2353\n",
            "Epoch 2/15 | Batch 200/1762 | Loss: 2.9963 | Avg: 3.0942 | LR: 0.000500 | GradNorm: 1.1819\n",
            "Epoch 2/15 | Batch 300/1762 | Loss: 3.1438 | Avg: 3.0728 | LR: 0.000500 | GradNorm: 2.2598\n",
            "Epoch 2/15 | Batch 400/1762 | Loss: 2.8736 | Avg: 3.0634 | LR: 0.000500 | GradNorm: 1.9272\n",
            "Epoch 2/15 | Batch 500/1762 | Loss: 2.8131 | Avg: 3.0563 | LR: 0.000500 | GradNorm: 1.2597\n",
            "Epoch 2/15 | Batch 600/1762 | Loss: 2.8633 | Avg: 3.0484 | LR: 0.000500 | GradNorm: 1.7072\n",
            "Epoch 2/15 | Batch 700/1762 | Loss: 3.0564 | Avg: 3.0395 | LR: 0.000500 | GradNorm: 1.4393\n",
            "Epoch 2/15 | Batch 800/1762 | Loss: 2.9661 | Avg: 3.0294 | LR: 0.000500 | GradNorm: 1.3802\n",
            "Epoch 2/15 | Batch 900/1762 | Loss: 3.1519 | Avg: 3.0217 | LR: 0.000500 | GradNorm: 1.5246\n",
            "Epoch 2/15 | Batch 1000/1762 | Loss: 2.9882 | Avg: 3.0131 | LR: 0.000500 | GradNorm: 1.2059\n",
            "Epoch 2/15 | Batch 1100/1762 | Loss: 2.9727 | Avg: 3.0047 | LR: 0.000500 | GradNorm: 1.6009\n",
            "Epoch 2/15 | Batch 1200/1762 | Loss: 3.1626 | Avg: 2.9993 | LR: 0.000500 | GradNorm: 1.6799\n",
            "Epoch 2/15 | Batch 1300/1762 | Loss: 2.7724 | Avg: 2.9910 | LR: 0.000500 | GradNorm: 1.2459\n",
            "Epoch 2/15 | Batch 1400/1762 | Loss: 2.9006 | Avg: 2.9818 | LR: 0.000500 | GradNorm: 1.5441\n",
            "Epoch 2/15 | Batch 1500/1762 | Loss: 2.9516 | Avg: 2.9733 | LR: 0.000500 | GradNorm: 1.3884\n",
            "Epoch 2/15 | Batch 1600/1762 | Loss: 2.7591 | Avg: 2.9654 | LR: 0.000500 | GradNorm: 1.6080\n",
            "Epoch 2/15 | Batch 1700/1762 | Loss: 2.7645 | Avg: 2.9596 | LR: 0.000500 | GradNorm: 1.3130\n",
            "\n",
            "============================================================\n",
            "Epoch 2/15 | Average Loss: 2.9549\n",
            "best loss found so far.\n",
            "============================================================\n",
            "\n",
            "Checkpoint saved to /content/transformer_model.pt\n",
            "\n",
            "Epoch 3/15 | Batch 100/1762 | Loss: 2.7449 | Avg: 2.6702 | LR: 0.000500 | GradNorm: 1.2213\n",
            "Epoch 3/15 | Batch 200/1762 | Loss: 2.9888 | Avg: 2.6846 | LR: 0.000500 | GradNorm: 1.5520\n",
            "Epoch 3/15 | Batch 300/1762 | Loss: 2.5171 | Avg: 2.6872 | LR: 0.000500 | GradNorm: 1.1696\n",
            "Epoch 3/15 | Batch 400/1762 | Loss: 2.6310 | Avg: 2.6829 | LR: 0.000500 | GradNorm: 1.5332\n",
            "Epoch 3/15 | Batch 500/1762 | Loss: 2.7910 | Avg: 2.6833 | LR: 0.000500 | GradNorm: 1.8475\n",
            "Epoch 3/15 | Batch 600/1762 | Loss: 2.8147 | Avg: 2.6819 | LR: 0.000500 | GradNorm: 1.5335\n",
            "Epoch 3/15 | Batch 700/1762 | Loss: 2.8869 | Avg: 2.6800 | LR: 0.000500 | GradNorm: 1.6787\n",
            "Epoch 3/15 | Batch 800/1762 | Loss: 2.6350 | Avg: 2.6771 | LR: 0.000500 | GradNorm: 1.2581\n",
            "Epoch 3/15 | Batch 900/1762 | Loss: 2.6103 | Avg: 2.6731 | LR: 0.000500 | GradNorm: 1.4407\n",
            "Epoch 3/15 | Batch 1000/1762 | Loss: 2.6289 | Avg: 2.6715 | LR: 0.000500 | GradNorm: 1.4059\n",
            "Epoch 3/15 | Batch 1100/1762 | Loss: 2.7065 | Avg: 2.6694 | LR: 0.000500 | GradNorm: 1.8122\n",
            "Epoch 3/15 | Batch 1200/1762 | Loss: 2.7014 | Avg: 2.6675 | LR: 0.000500 | GradNorm: 1.6719\n",
            "Epoch 3/15 | Batch 1300/1762 | Loss: 2.5229 | Avg: 2.6642 | LR: 0.000500 | GradNorm: 1.3870\n",
            "Epoch 3/15 | Batch 1400/1762 | Loss: 2.3905 | Avg: 2.6615 | LR: 0.000500 | GradNorm: 1.3913\n",
            "Epoch 3/15 | Batch 1500/1762 | Loss: 2.5390 | Avg: 2.6593 | LR: 0.000500 | GradNorm: 1.7191\n",
            "Epoch 3/15 | Batch 1600/1762 | Loss: 2.8267 | Avg: 2.6579 | LR: 0.000500 | GradNorm: 1.3804\n",
            "Epoch 3/15 | Batch 1700/1762 | Loss: 3.0037 | Avg: 2.6555 | LR: 0.000500 | GradNorm: 1.7594\n",
            "\n",
            "============================================================\n",
            "Epoch 3/15 | Average Loss: 2.6535\n",
            "best loss found so far.\n",
            "============================================================\n",
            "\n",
            "Checkpoint saved to /content/transformer_model.pt\n",
            "\n",
            "Epoch 4/15 | Batch 100/1762 | Loss: 2.4016 | Avg: 2.4420 | LR: 0.000500 | GradNorm: 1.2360\n",
            "Epoch 4/15 | Batch 200/1762 | Loss: 2.3782 | Avg: 2.4445 | LR: 0.000500 | GradNorm: 1.2926\n",
            "Epoch 4/15 | Batch 300/1762 | Loss: 2.5270 | Avg: 2.4524 | LR: 0.000500 | GradNorm: 1.3978\n",
            "Epoch 4/15 | Batch 400/1762 | Loss: 2.3632 | Avg: 2.4540 | LR: 0.000500 | GradNorm: 1.3949\n",
            "Epoch 4/15 | Batch 500/1762 | Loss: 2.5289 | Avg: 2.4536 | LR: 0.000500 | GradNorm: 1.6537\n",
            "Epoch 4/15 | Batch 600/1762 | Loss: 2.3161 | Avg: 2.4553 | LR: 0.000500 | GradNorm: 1.2718\n",
            "Epoch 4/15 | Batch 700/1762 | Loss: 2.5325 | Avg: 2.4585 | LR: 0.000500 | GradNorm: 2.5285\n",
            "Epoch 4/15 | Batch 800/1762 | Loss: 2.5176 | Avg: 2.4600 | LR: 0.000500 | GradNorm: 1.3899\n",
            "Epoch 4/15 | Batch 900/1762 | Loss: 2.4373 | Avg: 2.4615 | LR: 0.000500 | GradNorm: 1.1657\n",
            "Epoch 4/15 | Batch 1000/1762 | Loss: 2.6355 | Avg: 2.4632 | LR: 0.000500 | GradNorm: 1.3866\n",
            "Epoch 4/15 | Batch 1100/1762 | Loss: 2.3662 | Avg: 2.4650 | LR: 0.000500 | GradNorm: 1.6126\n",
            "Epoch 4/15 | Batch 1200/1762 | Loss: 2.3006 | Avg: 2.4664 | LR: 0.000500 | GradNorm: 1.4025\n",
            "Epoch 4/15 | Batch 1300/1762 | Loss: 2.5920 | Avg: 2.4691 | LR: 0.000500 | GradNorm: 1.4245\n",
            "Epoch 4/15 | Batch 1400/1762 | Loss: 2.4832 | Avg: 2.4695 | LR: 0.000500 | GradNorm: 1.4892\n",
            "Epoch 4/15 | Batch 1500/1762 | Loss: 2.4924 | Avg: 2.4680 | LR: 0.000500 | GradNorm: 1.5256\n",
            "Epoch 4/15 | Batch 1600/1762 | Loss: 2.5012 | Avg: 2.4677 | LR: 0.000500 | GradNorm: 1.3997\n",
            "Epoch 4/15 | Batch 1700/1762 | Loss: 2.5006 | Avg: 2.4664 | LR: 0.000500 | GradNorm: 1.5934\n",
            "\n",
            "============================================================\n",
            "Epoch 4/15 | Average Loss: 2.4659\n",
            "best loss found so far.\n",
            "============================================================\n",
            "\n",
            "Checkpoint saved to /content/transformer_model.pt\n",
            "\n",
            "Epoch 5/15 | Batch 100/1762 | Loss: 2.1621 | Avg: 2.3109 | LR: 0.000500 | GradNorm: 1.3882\n",
            "Epoch 5/15 | Batch 200/1762 | Loss: 2.2488 | Avg: 2.2999 | LR: 0.000500 | GradNorm: 1.1275\n",
            "Epoch 5/15 | Batch 300/1762 | Loss: 2.3073 | Avg: 2.2957 | LR: 0.000500 | GradNorm: 1.7646\n",
            "Epoch 5/15 | Batch 400/1762 | Loss: 2.2319 | Avg: 2.3030 | LR: 0.000500 | GradNorm: 1.3772\n",
            "Epoch 5/15 | Batch 500/1762 | Loss: 2.4114 | Avg: 2.3076 | LR: 0.000500 | GradNorm: 1.3552\n",
            "Epoch 5/15 | Batch 600/1762 | Loss: 2.2687 | Avg: 2.3133 | LR: 0.000500 | GradNorm: 1.3435\n",
            "Epoch 5/15 | Batch 700/1762 | Loss: 2.1858 | Avg: 2.3149 | LR: 0.000500 | GradNorm: 1.5208\n",
            "Epoch 5/15 | Batch 800/1762 | Loss: 2.2745 | Avg: 2.3179 | LR: 0.000500 | GradNorm: 1.7571\n",
            "Epoch 5/15 | Batch 900/1762 | Loss: 2.3969 | Avg: 2.3190 | LR: 0.000500 | GradNorm: 1.8719\n",
            "Epoch 5/15 | Batch 1000/1762 | Loss: 2.2345 | Avg: 2.3215 | LR: 0.000500 | GradNorm: 2.0126\n",
            "Epoch 5/15 | Batch 1100/1762 | Loss: 2.3371 | Avg: 2.3251 | LR: 0.000500 | GradNorm: 1.3613\n",
            "Epoch 5/15 | Batch 1200/1762 | Loss: 2.4138 | Avg: 2.3260 | LR: 0.000500 | GradNorm: 1.3614\n",
            "Epoch 5/15 | Batch 1300/1762 | Loss: 2.3075 | Avg: 2.3291 | LR: 0.000500 | GradNorm: 2.6180\n",
            "Epoch 5/15 | Batch 1400/1762 | Loss: 2.2812 | Avg: 2.3295 | LR: 0.000500 | GradNorm: 1.4020\n",
            "Epoch 5/15 | Batch 1500/1762 | Loss: 2.5189 | Avg: 2.3308 | LR: 0.000500 | GradNorm: 1.4372\n",
            "Epoch 5/15 | Batch 1600/1762 | Loss: 2.4724 | Avg: 2.3311 | LR: 0.000500 | GradNorm: 1.4317\n",
            "Epoch 5/15 | Batch 1700/1762 | Loss: 2.5715 | Avg: 2.3335 | LR: 0.000500 | GradNorm: 1.5437\n",
            "\n",
            "============================================================\n",
            "Epoch 5/15 | Average Loss: 2.3343\n",
            "best loss found so far.\n",
            "============================================================\n",
            "\n",
            "Checkpoint saved to /content/transformer_model.pt\n",
            "\n",
            "Epoch 6/15 | Batch 100/1762 | Loss: 2.1959 | Avg: 2.1925 | LR: 0.000500 | GradNorm: 1.3149\n",
            "Epoch 6/15 | Batch 200/1762 | Loss: 2.2317 | Avg: 2.1971 | LR: 0.000500 | GradNorm: 1.6839\n",
            "Epoch 6/15 | Batch 300/1762 | Loss: 2.1512 | Avg: 2.2041 | LR: 0.000500 | GradNorm: 1.4155\n",
            "Epoch 6/15 | Batch 400/1762 | Loss: 2.1320 | Avg: 2.2068 | LR: 0.000500 | GradNorm: 1.5950\n",
            "Epoch 6/15 | Batch 500/1762 | Loss: 2.2032 | Avg: 2.2089 | LR: 0.000500 | GradNorm: 1.3522\n",
            "Epoch 6/15 | Batch 600/1762 | Loss: 2.1242 | Avg: 2.2102 | LR: 0.000500 | GradNorm: 1.5307\n",
            "Epoch 6/15 | Batch 700/1762 | Loss: 2.0294 | Avg: 2.2137 | LR: 0.000500 | GradNorm: 1.2402\n",
            "Epoch 6/15 | Batch 800/1762 | Loss: 2.1060 | Avg: 2.2168 | LR: 0.000500 | GradNorm: 1.2579\n",
            "Epoch 6/15 | Batch 900/1762 | Loss: 2.2603 | Avg: 2.2202 | LR: 0.000500 | GradNorm: 1.5923\n",
            "Epoch 6/15 | Batch 1000/1762 | Loss: 2.3928 | Avg: 2.2217 | LR: 0.000500 | GradNorm: 1.5509\n",
            "Epoch 6/15 | Batch 1100/1762 | Loss: 2.2497 | Avg: 2.2252 | LR: 0.000500 | GradNorm: 1.4294\n",
            "Epoch 6/15 | Batch 1200/1762 | Loss: 2.1167 | Avg: 2.2280 | LR: 0.000500 | GradNorm: 2.3194\n",
            "Epoch 6/15 | Batch 1300/1762 | Loss: 2.3484 | Avg: 2.2308 | LR: 0.000500 | GradNorm: 1.5745\n",
            "Epoch 6/15 | Batch 1400/1762 | Loss: 2.1461 | Avg: 2.2334 | LR: 0.000500 | GradNorm: 1.2554\n",
            "Epoch 6/15 | Batch 1500/1762 | Loss: 2.4146 | Avg: 2.2346 | LR: 0.000500 | GradNorm: 1.4199\n",
            "Epoch 6/15 | Batch 1600/1762 | Loss: 2.3944 | Avg: 2.2373 | LR: 0.000500 | GradNorm: 1.8510\n",
            "Epoch 6/15 | Batch 1700/1762 | Loss: 2.2817 | Avg: 2.2384 | LR: 0.000500 | GradNorm: 1.8858\n",
            "\n",
            "============================================================\n",
            "Epoch 6/15 | Average Loss: 2.2391\n",
            "best loss found so far.\n",
            "============================================================\n",
            "\n",
            "Checkpoint saved to /content/transformer_model.pt\n",
            "\n",
            "Epoch 7/15 | Batch 100/1762 | Loss: 2.1578 | Avg: 2.0899 | LR: 0.000500 | GradNorm: 1.4037\n",
            "Epoch 7/15 | Batch 200/1762 | Loss: 2.0239 | Avg: 2.0981 | LR: 0.000500 | GradNorm: 1.3932\n",
            "Epoch 7/15 | Batch 300/1762 | Loss: 2.0974 | Avg: 2.1088 | LR: 0.000500 | GradNorm: 1.4711\n",
            "Epoch 7/15 | Batch 400/1762 | Loss: 2.1850 | Avg: 2.1152 | LR: 0.000500 | GradNorm: 1.5026\n",
            "Epoch 7/15 | Batch 500/1762 | Loss: 2.0898 | Avg: 2.1207 | LR: 0.000500 | GradNorm: 1.3241\n",
            "Epoch 7/15 | Batch 600/1762 | Loss: 2.1378 | Avg: 2.1254 | LR: 0.000500 | GradNorm: 1.6455\n",
            "Epoch 7/15 | Batch 700/1762 | Loss: 2.2060 | Avg: 2.1298 | LR: 0.000500 | GradNorm: 1.4869\n",
            "Epoch 7/15 | Batch 800/1762 | Loss: 2.1921 | Avg: 2.1349 | LR: 0.000500 | GradNorm: 1.5955\n",
            "Epoch 7/15 | Batch 900/1762 | Loss: 2.1200 | Avg: 2.1383 | LR: 0.000500 | GradNorm: 1.5178\n",
            "Epoch 7/15 | Batch 1000/1762 | Loss: 2.0418 | Avg: 2.1414 | LR: 0.000500 | GradNorm: 1.3337\n",
            "Epoch 7/15 | Batch 1100/1762 | Loss: 2.0656 | Avg: 2.1455 | LR: 0.000500 | GradNorm: 1.5415\n",
            "Epoch 7/15 | Batch 1200/1762 | Loss: 2.1088 | Avg: 2.1490 | LR: 0.000500 | GradNorm: 1.1873\n",
            "Epoch 7/15 | Batch 1300/1762 | Loss: 2.1465 | Avg: 2.1507 | LR: 0.000500 | GradNorm: 1.3590\n",
            "Epoch 7/15 | Batch 1400/1762 | Loss: 2.0463 | Avg: 2.1537 | LR: 0.000500 | GradNorm: 1.5528\n",
            "Epoch 7/15 | Batch 1500/1762 | Loss: 2.1711 | Avg: 2.1553 | LR: 0.000500 | GradNorm: 2.3589\n",
            "Epoch 7/15 | Batch 1600/1762 | Loss: 2.2269 | Avg: 2.1578 | LR: 0.000500 | GradNorm: 1.4325\n",
            "Epoch 7/15 | Batch 1700/1762 | Loss: 2.2743 | Avg: 2.1597 | LR: 0.000500 | GradNorm: 1.3501\n",
            "\n",
            "============================================================\n",
            "Epoch 7/15 | Average Loss: 2.1608\n",
            "best loss found so far.\n",
            "============================================================\n",
            "\n",
            "Checkpoint saved to /content/transformer_model.pt\n",
            "\n",
            "Epoch 8/15 | Batch 100/1762 | Loss: 2.0660 | Avg: 2.0377 | LR: 0.000500 | GradNorm: 1.7966\n",
            "Epoch 8/15 | Batch 200/1762 | Loss: 2.0359 | Avg: 2.0339 | LR: 0.000500 | GradNorm: 1.6039\n",
            "Epoch 8/15 | Batch 300/1762 | Loss: 2.0014 | Avg: 2.0400 | LR: 0.000500 | GradNorm: 1.2865\n",
            "Epoch 8/15 | Batch 400/1762 | Loss: 1.9854 | Avg: 2.0469 | LR: 0.000500 | GradNorm: 1.3821\n",
            "Epoch 8/15 | Batch 500/1762 | Loss: 2.1524 | Avg: 2.0535 | LR: 0.000500 | GradNorm: 1.4979\n",
            "Epoch 8/15 | Batch 600/1762 | Loss: 2.2233 | Avg: 2.0574 | LR: 0.000500 | GradNorm: 1.5617\n",
            "Epoch 8/15 | Batch 700/1762 | Loss: 2.0715 | Avg: 2.0615 | LR: 0.000500 | GradNorm: 1.5320\n",
            "Epoch 8/15 | Batch 800/1762 | Loss: 1.9668 | Avg: 2.0658 | LR: 0.000500 | GradNorm: 1.3668\n",
            "Epoch 8/15 | Batch 900/1762 | Loss: 2.0392 | Avg: 2.0699 | LR: 0.000500 | GradNorm: 1.2972\n",
            "Epoch 8/15 | Batch 1000/1762 | Loss: 2.0443 | Avg: 2.0742 | LR: 0.000500 | GradNorm: 1.3675\n",
            "Epoch 8/15 | Batch 1100/1762 | Loss: 2.0352 | Avg: 2.0770 | LR: 0.000500 | GradNorm: 1.4507\n",
            "Epoch 8/15 | Batch 1200/1762 | Loss: 2.2349 | Avg: 2.0805 | LR: 0.000500 | GradNorm: 1.7853\n",
            "Epoch 8/15 | Batch 1300/1762 | Loss: 2.2081 | Avg: 2.0840 | LR: 0.000500 | GradNorm: 1.6223\n",
            "Epoch 8/15 | Batch 1400/1762 | Loss: 2.1241 | Avg: 2.0862 | LR: 0.000500 | GradNorm: 1.4252\n",
            "Epoch 8/15 | Batch 1500/1762 | Loss: 2.0984 | Avg: 2.0894 | LR: 0.000500 | GradNorm: 1.3984\n",
            "Epoch 8/15 | Batch 1600/1762 | Loss: 2.1956 | Avg: 2.0924 | LR: 0.000500 | GradNorm: 1.4539\n",
            "Epoch 8/15 | Batch 1700/1762 | Loss: 2.1476 | Avg: 2.0942 | LR: 0.000500 | GradNorm: 1.5786\n",
            "\n",
            "============================================================\n",
            "Epoch 8/15 | Average Loss: 2.0958\n",
            "best loss found so far.\n",
            "============================================================\n",
            "\n",
            "Checkpoint saved to /content/transformer_model.pt\n",
            "\n",
            "Epoch 9/15 | Batch 100/1762 | Loss: 1.9555 | Avg: 1.9740 | LR: 0.000500 | GradNorm: 1.8930\n",
            "Epoch 9/15 | Batch 200/1762 | Loss: 1.9900 | Avg: 1.9789 | LR: 0.000500 | GradNorm: 1.8943\n",
            "Epoch 9/15 | Batch 300/1762 | Loss: 2.0032 | Avg: 1.9870 | LR: 0.000500 | GradNorm: 2.2472\n",
            "Epoch 9/15 | Batch 400/1762 | Loss: 2.0417 | Avg: 1.9918 | LR: 0.000500 | GradNorm: 1.6517\n",
            "Epoch 9/15 | Batch 500/1762 | Loss: 2.0372 | Avg: 1.9955 | LR: 0.000500 | GradNorm: 1.6272\n",
            "Epoch 9/15 | Batch 600/1762 | Loss: 1.9265 | Avg: 2.0008 | LR: 0.000500 | GradNorm: 1.7704\n",
            "Epoch 9/15 | Batch 700/1762 | Loss: 2.0178 | Avg: 2.0051 | LR: 0.000500 | GradNorm: 3.1278\n",
            "Epoch 9/15 | Batch 800/1762 | Loss: 2.1632 | Avg: 2.0102 | LR: 0.000500 | GradNorm: 1.5776\n",
            "Epoch 9/15 | Batch 900/1762 | Loss: 2.0529 | Avg: 2.0150 | LR: 0.000500 | GradNorm: 1.6366\n",
            "Epoch 9/15 | Batch 1000/1762 | Loss: 2.1906 | Avg: 2.0192 | LR: 0.000500 | GradNorm: 1.5749\n",
            "Epoch 9/15 | Batch 1100/1762 | Loss: 1.9837 | Avg: 2.0228 | LR: 0.000500 | GradNorm: 1.4990\n",
            "Epoch 9/15 | Batch 1200/1762 | Loss: 2.0251 | Avg: 2.0262 | LR: 0.000500 | GradNorm: 1.2975\n",
            "Epoch 9/15 | Batch 1300/1762 | Loss: 2.0757 | Avg: 2.0299 | LR: 0.000500 | GradNorm: 1.4794\n",
            "Epoch 9/15 | Batch 1400/1762 | Loss: 2.1047 | Avg: 2.0325 | LR: 0.000500 | GradNorm: 1.7193\n",
            "Epoch 9/15 | Batch 1500/1762 | Loss: 2.0895 | Avg: 2.0346 | LR: 0.000500 | GradNorm: 1.5789\n",
            "Epoch 9/15 | Batch 1600/1762 | Loss: 2.0684 | Avg: 2.0366 | LR: 0.000500 | GradNorm: 2.0012\n",
            "Epoch 9/15 | Batch 1700/1762 | Loss: 2.1673 | Avg: 2.0394 | LR: 0.000500 | GradNorm: 5.3916\n",
            "\n",
            "============================================================\n",
            "Epoch 9/15 | Average Loss: 2.0413\n",
            "best loss found so far.\n",
            "============================================================\n",
            "\n",
            "Checkpoint saved to /content/transformer_model.pt\n",
            "\n",
            "Epoch 10/15 | Batch 100/1762 | Loss: 1.9952 | Avg: 1.9215 | LR: 0.000500 | GradNorm: 1.8740\n",
            "Epoch 10/15 | Batch 200/1762 | Loss: 1.9336 | Avg: 1.9266 | LR: 0.000500 | GradNorm: 2.1407\n",
            "Epoch 10/15 | Batch 300/1762 | Loss: 1.9112 | Avg: 1.9341 | LR: 0.000500 | GradNorm: 1.3661\n",
            "Epoch 10/15 | Batch 400/1762 | Loss: 1.9932 | Avg: 1.9422 | LR: 0.000500 | GradNorm: 1.7758\n",
            "Epoch 10/15 | Batch 500/1762 | Loss: 2.0146 | Avg: 1.9467 | LR: 0.000500 | GradNorm: 1.9870\n",
            "Epoch 10/15 | Batch 600/1762 | Loss: 1.9543 | Avg: 1.9532 | LR: 0.000500 | GradNorm: 1.3872\n",
            "Epoch 10/15 | Batch 700/1762 | Loss: 1.9999 | Avg: 1.9582 | LR: 0.000500 | GradNorm: 1.7590\n",
            "Epoch 10/15 | Batch 800/1762 | Loss: 1.8124 | Avg: 1.9621 | LR: 0.000500 | GradNorm: 1.4309\n",
            "Epoch 10/15 | Batch 900/1762 | Loss: 2.0553 | Avg: 1.9659 | LR: 0.000500 | GradNorm: 1.5881\n",
            "Epoch 10/15 | Batch 1000/1762 | Loss: 2.0137 | Avg: 1.9697 | LR: 0.000500 | GradNorm: 1.3746\n",
            "Epoch 10/15 | Batch 1100/1762 | Loss: 2.0374 | Avg: 1.9730 | LR: 0.000500 | GradNorm: 1.5713\n",
            "Epoch 10/15 | Batch 1200/1762 | Loss: 1.9545 | Avg: 1.9768 | LR: 0.000500 | GradNorm: 1.3899\n",
            "Epoch 10/15 | Batch 1300/1762 | Loss: 2.0184 | Avg: 1.9805 | LR: 0.000500 | GradNorm: 2.2573\n",
            "Epoch 10/15 | Batch 1400/1762 | Loss: 1.9426 | Avg: 1.9838 | LR: 0.000500 | GradNorm: 1.5779\n",
            "Epoch 10/15 | Batch 1500/1762 | Loss: 2.0941 | Avg: 1.9874 | LR: 0.000500 | GradNorm: 1.4089\n",
            "Epoch 10/15 | Batch 1600/1762 | Loss: 1.9976 | Avg: 1.9901 | LR: 0.000500 | GradNorm: 1.3335\n",
            "Epoch 10/15 | Batch 1700/1762 | Loss: 2.0307 | Avg: 1.9926 | LR: 0.000500 | GradNorm: 1.3763\n",
            "\n",
            "============================================================\n",
            "Epoch 10/15 | Average Loss: 1.9942\n",
            "best loss found so far.\n",
            "============================================================\n",
            "\n",
            "Checkpoint saved to /content/transformer_model.pt\n",
            "\n",
            "Epoch 11/15 | Batch 100/1762 | Loss: 1.8627 | Avg: 1.8962 | LR: 0.000500 | GradNorm: 1.6156\n",
            "Epoch 11/15 | Batch 200/1762 | Loss: 1.9256 | Avg: 1.8928 | LR: 0.000500 | GradNorm: 1.5855\n",
            "Epoch 11/15 | Batch 300/1762 | Loss: 1.9900 | Avg: 1.8999 | LR: 0.000500 | GradNorm: 1.5822\n",
            "Epoch 11/15 | Batch 400/1762 | Loss: 1.8457 | Avg: 1.9059 | LR: 0.000500 | GradNorm: 1.4193\n",
            "Epoch 11/15 | Batch 500/1762 | Loss: 2.0011 | Avg: 1.9099 | LR: 0.000500 | GradNorm: 1.4111\n",
            "Epoch 11/15 | Batch 600/1762 | Loss: 1.9283 | Avg: 1.9150 | LR: 0.000500 | GradNorm: 1.2927\n",
            "Epoch 11/15 | Batch 700/1762 | Loss: 1.9011 | Avg: 1.9189 | LR: 0.000500 | GradNorm: 1.3586\n",
            "Epoch 11/15 | Batch 800/1762 | Loss: 2.0242 | Avg: 1.9224 | LR: 0.000500 | GradNorm: 1.5463\n",
            "Epoch 11/15 | Batch 900/1762 | Loss: 1.8946 | Avg: 1.9276 | LR: 0.000500 | GradNorm: 1.4207\n",
            "Epoch 11/15 | Batch 1000/1762 | Loss: 1.9973 | Avg: 1.9309 | LR: 0.000500 | GradNorm: 1.4632\n",
            "Epoch 11/15 | Batch 1100/1762 | Loss: 1.9415 | Avg: 1.9350 | LR: 0.000500 | GradNorm: 1.3498\n",
            "Epoch 11/15 | Batch 1200/1762 | Loss: 2.0418 | Avg: 1.9391 | LR: 0.000500 | GradNorm: 1.4385\n",
            "Epoch 11/15 | Batch 1300/1762 | Loss: 1.9598 | Avg: 1.9422 | LR: 0.000500 | GradNorm: 1.4605\n",
            "Epoch 11/15 | Batch 1400/1762 | Loss: 1.9057 | Avg: 1.9457 | LR: 0.000500 | GradNorm: 1.3077\n",
            "Epoch 11/15 | Batch 1500/1762 | Loss: 2.1040 | Avg: 1.9488 | LR: 0.000500 | GradNorm: 1.5873\n",
            "Epoch 11/15 | Batch 1600/1762 | Loss: 2.0209 | Avg: 1.9516 | LR: 0.000500 | GradNorm: 1.4857\n",
            "Epoch 11/15 | Batch 1700/1762 | Loss: 1.9218 | Avg: 1.9544 | LR: 0.000500 | GradNorm: 4.2923\n",
            "\n",
            "============================================================\n",
            "Epoch 11/15 | Average Loss: 1.9559\n",
            "best loss found so far.\n",
            "============================================================\n",
            "\n",
            "Checkpoint saved to /content/transformer_model.pt\n",
            "\n",
            "Epoch 12/15 | Batch 100/1762 | Loss: 1.9065 | Avg: 1.8511 | LR: 0.000500 | GradNorm: 1.6154\n",
            "Epoch 12/15 | Batch 200/1762 | Loss: 1.8540 | Avg: 1.8571 | LR: 0.000500 | GradNorm: 2.2766\n",
            "Epoch 12/15 | Batch 300/1762 | Loss: 1.9050 | Avg: 1.8614 | LR: 0.000500 | GradNorm: 1.9524\n",
            "Epoch 12/15 | Batch 400/1762 | Loss: 1.9076 | Avg: 1.8677 | LR: 0.000500 | GradNorm: 1.7806\n",
            "Epoch 12/15 | Batch 500/1762 | Loss: 1.9171 | Avg: 1.8746 | LR: 0.000500 | GradNorm: 1.9347\n",
            "Epoch 12/15 | Batch 600/1762 | Loss: 1.9370 | Avg: 1.8775 | LR: 0.000500 | GradNorm: 1.5051\n",
            "Epoch 12/15 | Batch 700/1762 | Loss: 1.9694 | Avg: 1.8811 | LR: 0.000500 | GradNorm: 1.8720\n",
            "Epoch 12/15 | Batch 800/1762 | Loss: 1.8978 | Avg: 1.8862 | LR: 0.000500 | GradNorm: 1.3048\n",
            "Epoch 12/15 | Batch 900/1762 | Loss: 1.9508 | Avg: 1.8908 | LR: 0.000500 | GradNorm: 1.3974\n",
            "Epoch 12/15 | Batch 1000/1762 | Loss: 1.7844 | Avg: 1.8949 | LR: 0.000500 | GradNorm: 1.0907\n",
            "Epoch 12/15 | Batch 1100/1762 | Loss: 1.8613 | Avg: 1.8985 | LR: 0.000500 | GradNorm: 1.7132\n",
            "Epoch 12/15 | Batch 1200/1762 | Loss: 1.9114 | Avg: 1.9016 | LR: 0.000500 | GradNorm: 1.5131\n",
            "Epoch 12/15 | Batch 1300/1762 | Loss: 2.0322 | Avg: 1.9054 | LR: 0.000500 | GradNorm: 1.8589\n",
            "Epoch 12/15 | Batch 1400/1762 | Loss: 2.0200 | Avg: 1.9090 | LR: 0.000500 | GradNorm: 1.7703\n",
            "Epoch 12/15 | Batch 1500/1762 | Loss: 1.9512 | Avg: 1.9128 | LR: 0.000500 | GradNorm: 1.3687\n",
            "Epoch 12/15 | Batch 1600/1762 | Loss: 2.0059 | Avg: 1.9161 | LR: 0.000500 | GradNorm: 1.3571\n",
            "Epoch 12/15 | Batch 1700/1762 | Loss: 2.0423 | Avg: 1.9193 | LR: 0.000500 | GradNorm: 1.3820\n",
            "\n",
            "============================================================\n",
            "Epoch 12/15 | Average Loss: 1.9205\n",
            "best loss found so far.\n",
            "============================================================\n",
            "\n",
            "Checkpoint saved to /content/transformer_model.pt\n",
            "\n",
            "Epoch 13/15 | Batch 100/1762 | Loss: 1.8693 | Avg: 1.8292 | LR: 0.000500 | GradNorm: 1.4790\n",
            "Epoch 13/15 | Batch 200/1762 | Loss: 1.7976 | Avg: 1.8319 | LR: 0.000500 | GradNorm: 1.3465\n",
            "Epoch 13/15 | Batch 300/1762 | Loss: 1.8761 | Avg: 1.8374 | LR: 0.000500 | GradNorm: 1.6003\n",
            "Epoch 13/15 | Batch 400/1762 | Loss: 1.8674 | Avg: 1.8432 | LR: 0.000500 | GradNorm: 1.8777\n",
            "Epoch 13/15 | Batch 500/1762 | Loss: 1.8426 | Avg: 1.8479 | LR: 0.000500 | GradNorm: 1.3060\n",
            "Epoch 13/15 | Batch 600/1762 | Loss: 1.8743 | Avg: 1.8528 | LR: 0.000500 | GradNorm: 1.7156\n",
            "Epoch 13/15 | Batch 700/1762 | Loss: 1.8702 | Avg: 1.8555 | LR: 0.000500 | GradNorm: 4.3174\n",
            "Epoch 13/15 | Batch 800/1762 | Loss: 1.8701 | Avg: 1.8596 | LR: 0.000500 | GradNorm: 1.5984\n",
            "Epoch 13/15 | Batch 900/1762 | Loss: 1.9765 | Avg: 1.8628 | LR: 0.000500 | GradNorm: 1.8279\n",
            "Epoch 13/15 | Batch 1000/1762 | Loss: 1.9531 | Avg: 1.8667 | LR: 0.000500 | GradNorm: 1.6187\n",
            "Epoch 13/15 | Batch 1100/1762 | Loss: 1.8510 | Avg: 1.8707 | LR: 0.000500 | GradNorm: 5.3353\n",
            "Epoch 13/15 | Batch 1200/1762 | Loss: 1.8669 | Avg: 1.8734 | LR: 0.000500 | GradNorm: 1.2632\n",
            "Epoch 13/15 | Batch 1300/1762 | Loss: 1.8836 | Avg: 1.8763 | LR: 0.000500 | GradNorm: 1.3767\n",
            "Epoch 13/15 | Batch 1400/1762 | Loss: 2.0160 | Avg: 1.8802 | LR: 0.000500 | GradNorm: 1.5716\n",
            "Epoch 13/15 | Batch 1500/1762 | Loss: 1.9712 | Avg: 1.8841 | LR: 0.000500 | GradNorm: 1.5092\n",
            "Epoch 13/15 | Batch 1600/1762 | Loss: 1.9524 | Avg: 1.8871 | LR: 0.000500 | GradNorm: 1.7003\n",
            "Epoch 13/15 | Batch 1700/1762 | Loss: 2.0457 | Avg: 1.8895 | LR: 0.000500 | GradNorm: 6.3579\n",
            "\n",
            "============================================================\n",
            "Epoch 13/15 | Average Loss: 1.8913\n",
            "best loss found so far.\n",
            "============================================================\n",
            "\n",
            "Checkpoint saved to /content/transformer_model.pt\n",
            "\n",
            "Epoch 14/15 | Batch 100/1762 | Loss: 1.7780 | Avg: 1.8084 | LR: 0.000500 | GradNorm: 2.7500\n",
            "Epoch 14/15 | Batch 200/1762 | Loss: 1.8523 | Avg: 1.8081 | LR: 0.000500 | GradNorm: 2.4900\n",
            "Epoch 14/15 | Batch 300/1762 | Loss: 1.8001 | Avg: 1.8132 | LR: 0.000500 | GradNorm: 1.8599\n",
            "Epoch 14/15 | Batch 400/1762 | Loss: 1.8548 | Avg: 1.8158 | LR: 0.000500 | GradNorm: 1.4458\n",
            "Epoch 14/15 | Batch 500/1762 | Loss: 1.8591 | Avg: 1.8211 | LR: 0.000500 | GradNorm: 1.9330\n",
            "Epoch 14/15 | Batch 600/1762 | Loss: 1.8766 | Avg: 1.8256 | LR: 0.000500 | GradNorm: 1.3582\n",
            "Epoch 14/15 | Batch 700/1762 | Loss: 1.7981 | Avg: 1.8305 | LR: 0.000500 | GradNorm: 1.4185\n",
            "Epoch 14/15 | Batch 800/1762 | Loss: 1.9011 | Avg: 1.8349 | LR: 0.000500 | GradNorm: 1.4002\n",
            "Epoch 14/15 | Batch 900/1762 | Loss: 1.9226 | Avg: 1.8382 | LR: 0.000500 | GradNorm: 1.8443\n",
            "Epoch 14/15 | Batch 1000/1762 | Loss: 1.8881 | Avg: 1.8415 | LR: 0.000500 | GradNorm: 1.2189\n",
            "Epoch 14/15 | Batch 1100/1762 | Loss: 1.8982 | Avg: 1.8447 | LR: 0.000500 | GradNorm: 1.4286\n",
            "Epoch 14/15 | Batch 1200/1762 | Loss: 1.9072 | Avg: 1.8484 | LR: 0.000500 | GradNorm: 3.0660\n",
            "Epoch 14/15 | Batch 1300/1762 | Loss: 1.9600 | Avg: 1.8520 | LR: 0.000500 | GradNorm: 1.5187\n",
            "Epoch 14/15 | Batch 1400/1762 | Loss: 1.8860 | Avg: 1.8554 | LR: 0.000500 | GradNorm: 1.3859\n",
            "Epoch 14/15 | Batch 1500/1762 | Loss: 1.8397 | Avg: 1.8582 | LR: 0.000500 | GradNorm: 1.3336\n",
            "Epoch 14/15 | Batch 1600/1762 | Loss: 1.8129 | Avg: 1.8610 | LR: 0.000500 | GradNorm: 2.6041\n",
            "Epoch 14/15 | Batch 1700/1762 | Loss: 1.9372 | Avg: 1.8638 | LR: 0.000500 | GradNorm: 1.3564\n",
            "\n",
            "============================================================\n",
            "Epoch 14/15 | Average Loss: 1.8660\n",
            "best loss found so far.\n",
            "============================================================\n",
            "\n",
            "Checkpoint saved to /content/transformer_model.pt\n",
            "\n",
            "Epoch 15/15 | Batch 100/1762 | Loss: 1.7917 | Avg: 1.7728 | LR: 0.000500 | GradNorm: 1.1442\n",
            "Epoch 15/15 | Batch 200/1762 | Loss: 1.7568 | Avg: 1.7832 | LR: 0.000500 | GradNorm: 1.1286\n",
            "Epoch 15/15 | Batch 300/1762 | Loss: 1.8447 | Avg: 1.7882 | LR: 0.000500 | GradNorm: 1.4024\n",
            "Epoch 15/15 | Batch 400/1762 | Loss: 1.8387 | Avg: 1.7934 | LR: 0.000500 | GradNorm: 1.6406\n",
            "Epoch 15/15 | Batch 500/1762 | Loss: 1.8203 | Avg: 1.7978 | LR: 0.000500 | GradNorm: 1.4248\n",
            "Epoch 15/15 | Batch 600/1762 | Loss: 1.8899 | Avg: 1.8035 | LR: 0.000500 | GradNorm: 4.0810\n",
            "Epoch 15/15 | Batch 700/1762 | Loss: 1.8564 | Avg: 1.8080 | LR: 0.000500 | GradNorm: 1.6106\n",
            "Epoch 15/15 | Batch 800/1762 | Loss: 1.8113 | Avg: 1.8122 | LR: 0.000500 | GradNorm: 1.1690\n",
            "Epoch 15/15 | Batch 900/1762 | Loss: 1.7905 | Avg: 1.8162 | LR: 0.000500 | GradNorm: 1.4355\n",
            "Epoch 15/15 | Batch 1000/1762 | Loss: 1.9394 | Avg: 1.8200 | LR: 0.000500 | GradNorm: 2.2577\n",
            "Epoch 15/15 | Batch 1100/1762 | Loss: 1.8386 | Avg: 1.8227 | LR: 0.000500 | GradNorm: 1.2362\n",
            "Epoch 15/15 | Batch 1200/1762 | Loss: 1.8073 | Avg: 1.8259 | LR: 0.000500 | GradNorm: 1.3476\n",
            "Epoch 15/15 | Batch 1300/1762 | Loss: 1.9566 | Avg: 1.8290 | LR: 0.000500 | GradNorm: 1.4198\n",
            "Epoch 15/15 | Batch 1400/1762 | Loss: 1.9054 | Avg: 1.8317 | LR: 0.000500 | GradNorm: 1.4333\n",
            "Epoch 15/15 | Batch 1500/1762 | Loss: 1.9479 | Avg: 1.8346 | LR: 0.000500 | GradNorm: 1.4648\n",
            "Epoch 15/15 | Batch 1600/1762 | Loss: 1.9013 | Avg: 1.8376 | LR: 0.000500 | GradNorm: 1.6732\n",
            "Epoch 15/15 | Batch 1700/1762 | Loss: 1.7990 | Avg: 1.8402 | LR: 0.000500 | GradNorm: 1.2186\n",
            "\n",
            "============================================================\n",
            "Epoch 15/15 | Average Loss: 1.8417\n",
            "best loss found so far.\n",
            "============================================================\n",
            "\n",
            "Checkpoint saved to /content/transformer_model.pt\n",
            "\n",
            "\n",
            "Training completed!\n",
            "Best loss achieved: 1.8417\n",
            "Final model saved to /content/transformer_model.pt\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m updating run metadata (0.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m updating run metadata (0.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£½\u001b[0m updating run metadata (0.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¾\u001b[0m updating run metadata (0.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£·\u001b[0m uploading wandb-summary.json 240B/240B (0.3s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£·\u001b[0m uploading history steps 26416-26444, summary, console lines 372-383 ...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£·\u001b[0m uploading config.yaml 2.0KB/2.0KB (0.1s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¯\u001b[0m uploading wandb-summary.json 240B/240B (0.3s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¯\u001b[0m uploading history steps 26416-26444, summary, console lines 372-383 ...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¯\u001b[0m uploading config.yaml 2.0KB/2.0KB (0.1s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£Ÿ\u001b[0m uploading wandb-summary.json 240B/240B (0.3s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£Ÿ\u001b[0m uploading history steps 26416-26444, summary, console lines 372-383 ...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£Ÿ\u001b[0m uploading config.yaml 2.0KB/2.0KB (0.1s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¡¿\u001b[0m uploading wandb-summary.json 240B/240B (0.3s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¡¿\u001b[0m uploading history steps 26416-26444, summary, console lines 372-383 ...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¡¿\u001b[0m uploading config.yaml 2.0KB/2.0KB (0.1s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m uploading wandb-summary.json 240B/240B (0.3s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m uploading history steps 26416-26444, summary, console lines 372-383 ...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m uploading config.yaml 2.0KB/2.0KB (0.1s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m uploading data (0.1s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    batch_loss â–ˆâ–ˆâ–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–â–â–‚\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         epoch â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    epoch_loss â–ˆâ–„â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     grad_norm â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–ƒâ–ƒâ–‚â–‚â–‡â–‚â–ƒâ–ƒâ–â–„â–‚â–…â–‚â–‚â–†â–â–‚â–‚â–‚â–ˆâ–†â–‚â–ƒâ–‚â–‚â–ƒâ–ƒâ–‚â–‚\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: learning_rate â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    batch_loss 1.75523\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         epoch 15\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    epoch_loss 1.84174\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   global_step 26430\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     grad_norm 4.40722\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: learning_rate 0.0005\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33mgood-meadow-1\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/tahasanhabib83-rajshahi-university-of-engineering-technology/transformer-sql-fixed/runs/8v0l0188\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/tahasanhabib83-rajshahi-university-of-engineering-technology/transformer-sql-fixed\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251214_135359-8v0l0188/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import random\n",
        "import os\n",
        "from inference import SqlInference\n",
        "\n",
        "def load_random_sample(filepath):\n",
        "    if not os.path.exists(filepath):\n",
        "        print(f\"Error: Data file not found at {filepath}\")\n",
        "        return None\n",
        "\n",
        "    with open(filepath, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "        if not lines:\n",
        "            return None\n",
        "        return json.loads(random.choice(lines))\n",
        "\n",
        "def main():\n",
        "    # Initialize the inference class\n",
        "    infer = SqlInference()\n",
        "\n",
        "    # Path to your training data\n",
        "    # Adjust this if your data is in a different location\n",
        "    data_path = \"MyTrials/train.json\"\n",
        "\n",
        "    print(f\"Attempting to load sample from {data_path}...\")\n",
        "    sample = load_random_sample(data_path)\n",
        "\n",
        "    if sample:\n",
        "        question = sample['question']\n",
        "        table = sample['table']\n",
        "        human_readable_sql = sample.get('sql', {}).get('human_readable', 'N/A')\n",
        "\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"SAMPLE FROM DATASET\")\n",
        "        print(\"=\"*50)\n",
        "        print(f\"Question: {question}\")\n",
        "        print(f\"Expected SQL: {human_readable_sql}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        # Generate prediction\n",
        "        generated_sql = infer.predict(question, table)\n",
        "\n",
        "        print(f\"Generated SQL: {generated_sql}\")\n",
        "        print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "    else:\n",
        "        # Fallback to manual example if file not found\n",
        "        print(\"Could not load data file. Using manual example instead.\")\n",
        "        question = \"How many players are from United States\"\n",
        "        table = {\n",
        "            \"header\": [\"Player\", \"Nationality\"],\n",
        "            \"rows\": [\n",
        "                [\"Shawn Respert\", \"United States\"],\n",
        "                [\"Aleksandar Radojevi\", \"Serbia\"],\n",
        "                [\"Quentin Richardson\", \"United States\"]\n",
        "            ]\n",
        "        }\n",
        "        print(\"-\" * 50)\n",
        "        print(f\"Question: {question}\")\n",
        "        print(\"-\" * 50)\n",
        "        generated_sql = infer.predict(question, table)\n",
        "        print(f\"Generated SQL: {generated_sql}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZxeS0Xcna4F",
        "outputId": "fc5afcb6-96cb-440e-9a00-e4b3d38ada9f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model loaded from /content/transformer_model.pt\n",
            "Attempting to load sample from MyTrials/train.json...\n",
            "\n",
            "==================================================\n",
            "SAMPLE FROM DATASET\n",
            "==================================================\n",
            "Question: what's theÂ resultÂ withÂ recordÂ being 0â€“1\n",
            "Expected SQL: SELECT Result FROM table WHERE Record = 0â€“1\n",
            "--------------------------------------------------\n",
            "Encoder input: The question is: what's theÂ resultÂ withÂ recordÂ being 0â€“1 table: Week | Date | Opponent | Result | Record | Game Site | Attendance\n",
            "1 | September 13, 1964 | at Minnesota Vikings | L 24â€“34 | 0â€“1 | Metrop...\n",
            "Generated SQL: SELECT Result FROM table WHERE Record = L 4â€“1\n",
            "==================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "eCDFfYpkoNDD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}